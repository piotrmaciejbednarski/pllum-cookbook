{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Web UI for PLLuM-8x7B-chat GGUF with Gradio\n",
    "\n",
    "This notebook demonstrates how to create a simple web interface for the PLLuM-8x7B-chat model using Gradio. The interface will allow users to interact with the model in a chat-like manner, adjust generation parameters, and compare different models.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Download at least one quantized model from [Hugging Face](https://huggingface.co/piotrmaciejbednarski/PLLuM-8x7B-chat-GGUF)\n",
    "- Install required packages: `pip install llama-cpp-python gradio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install llama-cpp-python gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Set Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gradio as gr\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Define available models - update paths to match your setup\n",
    "models = {\n",
    "    \"PLLuM-8x7B-chat-q4_k_m\": \"../models/PLLuM-8x7B-chat-gguf-q4_k_m.gguf\",\n",
    "    \"PLLuM-8x7B-chat-q3_k_m\": \"../models/PLLuM-8x7B-chat-gguf-q3_k_m.gguf\",\n",
    "    \"PLLuM-8x7B-chat-iq3_s\": \"../models/PLLuM-8x7B-chat-gguf-iq3_s.gguf\"\n",
    "}\n",
    "\n",
    "# Check which models are available\n",
    "available_models = {}\n",
    "for name, path in models.items():\n",
    "    if os.path.exists(path):\n",
    "        available_models[name] = path\n",
    "        print(f\"✅ {name} found at {path}\")\n",
    "    else:\n",
    "        print(f\"❌ {name} not found at {path}\")\n",
    "\n",
    "if not available_models:\n",
    "    raise FileNotFoundError(\"No models found. Please download at least one model.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(available_models)} model(s) for the web UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Model Loading Function\n",
    "\n",
    "Let's create a function to load a model based on user selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to keep track of the loaded model\n",
    "loaded_model = None\n",
    "loaded_model_name = None\n",
    "\n",
    "def load_model(model_name, n_threads=8, n_ctx=2048):\n",
    "    \"\"\"Load a model based on the name and return it.\"\"\"\n",
    "    global loaded_model, loaded_model_name\n",
    "    \n",
    "    # If the requested model is already loaded, return it\n",
    "    if loaded_model is not None and loaded_model_name == model_name:\n",
    "        return loaded_model, f\"Model '{model_name}' is already loaded.\"\n",
    "    \n",
    "    # If a different model is loaded, unload it first\n",
    "    if loaded_model is not None:\n",
    "        del loaded_model\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    # Check if the model exists\n",
    "    if model_name not in available_models:\n",
    "        return None, f\"Model '{model_name}' not found. Available models: {', '.join(available_models.keys())}\"\n",
    "    \n",
    "    # Load the model\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        model_path = available_models[model_name]\n",
    "        \n",
    "        llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=n_ctx,\n",
    "            n_threads=n_threads,\n",
    "            n_batch=512,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        loaded_model = llm\n",
    "        loaded_model_name = model_name\n",
    "        \n",
    "        return llm, f\"Model '{model_name}' loaded successfully in {load_time:.2f} seconds.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Error loading model '{model_name}': {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Chat Interface Logic\n",
    "\n",
    "Let's implement the chat functionality to handle conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(history, system_prompt):\n",
    "    \"\"\"Format the chat history into a prompt for the model.\"\"\"\n",
    "    prompt = f\"Instrukcja: {system_prompt}\\n\\n\"\n",
    "    \n",
    "    for user_msg, bot_msg in history:\n",
    "        prompt += f\"Użytkownik: {user_msg}\\n\"\n",
    "        if bot_msg:\n",
    "            prompt += f\"Asystent: {bot_msg}\\n\"\n",
    "    \n",
    "    # Add the final user message prefix for the bot's response\n",
    "    prompt += \"Asystent: \"\n",
    "    return prompt\n",
    "\n",
    "def chat_with_model(message, history, system_prompt, model_name, temperature, max_tokens, top_p, top_k, repeat_penalty):\n",
    "    \"\"\"Process a chat message and return the model's response.\"\"\"\n",
    "    # Load the selected model if not already loaded\n",
    "    llm, status = load_model(model_name)\n",
    "    if llm is None:\n",
    "        return status\n",
    "    \n",
    "    # Format the prompt with history\n",
    "    prompt = format_chat_prompt(history, system_prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repeat_penalty=repeat_penalty,\n",
    "            stop=[\"\\nUżytkownik:\", \"\\nUser:\"]  # Stop generating at new user message\n",
    "        )\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        assistant_response = output[\"choices\"][0][\"text\"]\n",
    "        \n",
    "        # Add generation stats\n",
    "        tokens_generated = len(assistant_response.split())\n",
    "        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0\n",
    "        stats = f\"\\n\\n_Generated {tokens_generated} tokens in {generation_time:.2f} seconds ({tokens_per_second:.2f} tokens/sec)_\"\n",
    "        \n",
    "        return assistant_response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the Gradio Web Interface\n",
    "\n",
    "Now, let's build the Gradio interface with chat functionality and parameter controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_web_ui():\n",
    "    \"\"\"Create and launch the Gradio web interface.\"\"\"\n",
    "    # Default system prompt\n",
    "    default_system_prompt = (\n",
    "        \"Jesteś pomocnym, uprzejmym i dokładnym asystentem AI o imieniu PLLuM, \"\n",
    "        \"stworzonym przez polskie Ministerstwo Cyfryzacji. \"\n",
    "        \"Odpowiadasz na pytania użytkownika w języku polskim, chyba że zostaniesz poproszony o inny język. \"\n",
    "        \"Twoje odpowiedzi są zwięzłe, merytoryczne i pomocne.\"\n",
    "    )\n",
    "    \n",
    "    # Define interface\n",
    "    with gr.Blocks(title=\"PLLuM Chat\") as demo:\n",
    "        gr.Markdown(\"# PLLuM Chat Interface\")\n",
    "        gr.Markdown(\"Talk with the PLLuM-8x7B-chat model in Polish or English.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                # Chat interface\n",
    "                chatbot = gr.Chatbot(height=500, label=\"Conversation\")\n",
    "                msg = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your message\")\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"Clear Conversation\")\n",
    "                \n",
    "                # System prompt\n",
    "                system_prompt = gr.Textbox(\n",
    "                    value=\"\".join(default_system_prompt),\n",
    "                    label=\"System Prompt\",\n",
    "                    lines=3\n",
    "                )\n",
    "                \n",
    "            with gr.Column(scale=1):\n",
    "                # Model selection and parameters\n",
    "                model_dropdown = gr.Dropdown(\n",
    "                    choices=list(available_models.keys()),\n",
    "                    value=list(available_models.keys())[0] if available_models else None,\n",
    "                    label=\"Select Model\"\n",
    "                )\n",
    "                \n",
    "                with gr.Accordion(\"Generation Parameters\", open=True):\n",
    "                    temperature = gr.Slider(\n",
    "                        minimum=0.0, maximum=2.0, value=0.7, step=0.05,\n",
    "                        label=\"Temperature\",\n",
    "                        info=\"Higher values = more creative, lower values = more deterministic\"\n",
    "                    )\n",
    "                    \n",
    "                    max_tokens = gr.Slider(\n",
    "                        minimum=64, maximum=2048, value=512, step=64,\n",
    "                        label=\"Max Tokens\",\n",
    "                        info=\"Maximum length of generated response\"\n",
    "                    )\n",
    "                    \n",
    "                    top_p = gr.Slider(\n",
    "                        minimum=0.0, maximum=1.0, value=0.95, step=0.05,\n",
    "                        label=\"Top-p\",\n",
    "                        info=\"Nucleus sampling: consider tokens with top_p cumulative probability\"\n",
    "                    )\n",
    "                    \n",
    "                    top_k = gr.Slider(\n",
    "                        minimum=1, maximum=100, value=50, step=1,\n",
    "                        label=\"Top-k\",\n",
    "                        info=\"Consider top k most likely tokens at each step\"\n",
    "                    )\n",
    "                    \n",
    "                    repeat_penalty = gr.Slider(\n",
    "                        minimum=1.0, maximum=2.0, value=1.1, step=0.05,\n",
    "                        label=\"Repeat Penalty\",\n",
    "                        info=\"Higher values discourage repetition\"\n",
    "                    )\n",
    "                \n",
    "                # Model information\n",
    "                model_info = gr.Textbox(label=\"Model Status\", interactive=False)\n",
    "        \n",
    "        # Load the initial model\n",
    "        demo.load(lambda: f\"Please select a model and click 'Load Model'\", outputs=model_info)\n",
    "        \n",
    "        # Event handlers\n",
    "        load_btn = gr.Button(\"Load Model\")\n",
    "        load_btn.click(\n",
    "            lambda model: load_model(model)[1],\n",
    "            inputs=model_dropdown,\n",
    "            outputs=model_info\n",
    "        )\n",
    "        \n",
    "        # Chat submission events\n",
    "        def respond(message, chat_history, system_prompt, model_name, temperature, max_tokens, top_p, top_k, repeat_penalty):\n",
    "            if not message.strip():\n",
    "                return chat_history\n",
    "            \n",
    "            # Add user message to history\n",
    "            chat_history.append((message, None))\n",
    "            yield chat_history\n",
    "            \n",
    "            # Get model response\n",
    "            response = chat_with_model(\n",
    "                message, chat_history[:-1], system_prompt, model_name,\n",
    "                temperature, max_tokens, top_p, top_k, repeat_penalty\n",
    "            )\n",
    "            \n",
    "            # Update history with bot response\n",
    "            chat_history[-1] = (message, response)\n",
    "            yield chat_history\n",
    "        \n",
    "        submit_btn.click(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, system_prompt, model_dropdown, temperature, max_tokens, top_p, top_k, repeat_penalty],\n",
    "            outputs=chatbot,\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, system_prompt, model_dropdown, temperature, max_tokens, top_p, top_k, repeat_penalty],\n",
    "            outputs=chatbot,\n",
    "            show_progress=True\n",
    "        ).then(\n",
    "            lambda: \"\",  # Clear the message box after sending\n",
    "            outputs=msg\n",
    "        )\n",
    "        \n",
    "        # Clear conversation button\n",
    "        clear_btn.click(\n",
    "            lambda: [],\n",
    "            outputs=chatbot\n",
    "        )\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch the Web Interface\n",
    "\n",
    "Let's launch the web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and launch the interface\n",
    "demo = create_web_ui()\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Basic Translation Interface\n",
    "\n",
    "Let's also create a simpler interface specifically for translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, from_lang, to_lang, model_name, temperature=0.7):\n",
    "    \"\"\"Translate text using the PLLuM model.\"\"\"\n",
    "    # Load model\n",
    "    llm, status = load_model(model_name)\n",
    "    if llm is None:\n",
    "        return status\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"Przetłumacz poniższy tekst z języka {from_lang} na język {to_lang}:\\n\\n'{text}'\\n\\nTłumaczenie:\"\n",
    "    \n",
    "    # Generate translation\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repeat_penalty=1.1\n",
    "        )\n",
    "        \n",
    "        return output[\"choices\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error during translation: {str(e)}\"\n",
    "\n",
    "# Create translation interface\n",
    "with gr.Blocks(title=\"PLLuM Translator\") as translator_demo:\n",
    "    gr.Markdown(\"# PLLuM Translation Interface\")\n",
    "    gr.Markdown(\"Translate text between Polish and other languages using the PLLuM-8x7B-chat model.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = gr.Textbox(label=\"Input Text\", lines=5, placeholder=\"Enter text to translate...\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                from_lang = gr.Dropdown(\n",
    "                    choices=[\"polskiego\", \"angielskiego\", \"niemieckiego\", \"francuskiego\", \"hiszpańskiego\", \"włoskiego\", \"rosyjskiego\"],\n",
    "                    value=\"polskiego\",\n",
    "                    label=\"From Language\"\n",
    "                )\n",
    "                \n",
    "                to_lang = gr.Dropdown(\n",
    "                    choices=[\"polski\", \"angielski\", \"niemiecki\", \"francuski\", \"hiszpański\", \"włoski\", \"rosyjski\"],\n",
    "                    value=\"angielski\",\n",
    "                    label=\"To Language\"\n",
    "                )\n",
    "            \n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=list(available_models.keys()),\n",
    "                value=list(available_models.keys())[0] if available_models else None,\n",
    "                label=\"Select Model\"\n",
    "            )\n",
    "            \n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.0, maximum=1.0, value=0.3, step=0.1,\n",
    "                label=\"Temperature\",\n",
    "                info=\"Lower values recommended for translation\"\n",
    "            )\n",
    "            \n",
    "            translate_btn = gr.Button(\"Translate\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(label=\"Translation Result\", lines=5)\n",
    "            model_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
    "    \n",
    "    # Load the initial model\n",
    "    translator_demo.load(lambda: f\"Please click 'Load Model' to begin\", outputs=model_status)\n",
    "    \n",
    "    # Load model button\n",
    "    load_model_btn = gr.Button(\"Load Model\")\n",
    "    load_model_btn.click(\n",
    "        lambda model: load_model(model)[1],\n",
    "        inputs=model_dropdown,\n",
    "        outputs=model_status\n",
    "    )\n",
    "    \n",
    "    # Translate button\n",
    "    translate_btn.click(\n",
    "        translate_text,\n",
    "        inputs=[input_text, from_lang, to_lang, model_dropdown, temperature],\n",
    "        outputs=output_text\n",
    "    )\n",
    "\n",
    "# Launch the translation interface\n",
    "translator_demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating a Stand-alone Version\n",
    "\n",
    "Here's how you can create a standalone Python script that you can run from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../web_ui.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import gradio as gr\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Global variable to keep track of the loaded model\n",
    "loaded_model = None\n",
    "loaded_model_name = None\n",
    "\n",
    "def load_model(model_path, n_threads=8, n_ctx=2048):\n",
    "    \"\"\"Load a model from the given path.\"\"\"\n",
    "    global loaded_model, loaded_model_name\n",
    "    \n",
    "    model_name = os.path.basename(model_path)\n",
    "    \n",
    "    # If the requested model is already loaded, return it\n",
    "    if loaded_model is not None and loaded_model_name == model_name:\n",
    "        return loaded_model, f\"Model '{model_name}' is already loaded.\"\n",
    "    \n",
    "    # If a different model is loaded, unload it first\n",
    "    if loaded_model is not None:\n",
    "        del loaded_model\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    # Check if the model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        return None, f\"Model file not found at '{model_path}'\"\n",
    "    \n",
    "    # Load the model\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=n_ctx,\n",
    "            n_threads=n_threads,\n",
    "            n_batch=512,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        loaded_model = llm\n",
    "        loaded_model_name = model_name\n",
    "        \n",
    "        return llm, f\"Model '{model_name}' loaded successfully in {load_time:.2f} seconds.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Error loading model: {str(e)}\"\n",
    "\n",
    "def format_chat_prompt(history, system_prompt):\n",
    "    \"\"\"Format the chat history into a prompt for the model.\"\"\"\n",
    "    prompt = f\"Instrukcja: {system_prompt}\\n\\n\"\n",
    "    \n",
    "    for user_msg, bot_msg in history:\n",
    "        prompt += f\"Użytkownik: {user_msg}\\n\"\n",
    "        if bot_msg:\n",
    "            prompt += f\"Asystent: {bot_msg}\\n\"\n",
    "    \n",
    "    # Add the final user message prefix for the bot's response\n",
    "    prompt += \"Asystent: \"\n",
    "    return prompt\n",
    "\n",
    "def chat_with_model(message, history, system_prompt, temperature, max_tokens, top_p, top_k, repeat_penalty):\n",
    "    \"\"\"Process a chat message and return the model's response.\"\"\"\n",
    "    global loaded_model\n",
    "    \n",
    "    if loaded_model is None:\n",
    "        return \"No model loaded. Please load a model first.\"\n",
    "    \n",
    "    # Format the prompt with history\n",
    "    prompt = format_chat_prompt(history, system_prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        output = loaded_model(\n",
    "            prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repeat_penalty=repeat_penalty,\n",
    "            stop=[\"\\nUżytkownik:\", \"\\nUser:\"]  # Stop generating at new user message\n",
    "        )\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        assistant_response = output[\"choices\"][0][\"text\"]\n",
    "        \n",
    "        return assistant_response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "def create_web_ui():\n",
    "    \"\"\"Create and launch the Gradio web interface.\"\"\"\n",
    "    # Default system prompt\n",
    "    default_system_prompt = (\n",
    "        \"Jesteś pomocnym, uprzejmym i dokładnym asystentem AI o imieniu PLLuM, \"\n",
    "        \"stworzonym przez polskie Ministerstwo Cyfryzacji. \"\n",
    "        \"Odpowiadasz na pytania użytkownika w języku polskim, chyba że zostaniesz poproszony o inny język. \"\n",
    "        \"Twoje odpowiedzi są zwięzłe, merytoryczne i pomocne.\"\n",
    "    )\n",
    "    \n",
    "    # Define interface\n",
    "    with gr.Blocks(title=\"PLLuM Chat\") as demo:\n",
    "        gr.Markdown(\"# PLLuM Chat Interface\")\n",
    "        gr.Markdown(\"Talk with the PLLuM-8x7B-chat model in Polish or English.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                # Chat interface\n",
    "                chatbot = gr.Chatbot(height=500, label=\"Conversation\")\n",
    "                msg = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your message\")\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"Clear Conversation\")\n",
    "                \n",
    "                # System prompt\n",
    "                system_prompt = gr.Textbox(\n",
    "                    value=\"\".join(default_system_prompt),\n",
    "                    label=\"System Prompt\",\n",
    "                    lines=3\n",
    "                )\n",
    "                \n",
    "            with gr.Column(scale=1):\n",
    "                # Model path\n",
    "                model_path = gr.Textbox(\n",
    "                    label=\"Model Path\",\n",
    "                    placeholder=\"Enter the full path to the model file\"\n",
    "                )\n",
    "                \n",
    "                with gr.Accordion(\"Generation Parameters\", open=True):\n",
    "                    temperature = gr.Slider(\n",
    "                        minimum=0.0, maximum=2.0, value=0.7, step=0.05,\n",
    "                        label=\"Temperature\",\n",
    "                        info=\"Higher values = more creative, lower values = more deterministic\"\n",
    "                    )\n",
    "                    \n",
    "                    max_tokens = gr.Slider(\n",
    "                        minimum=64, maximum=2048, value=512, step=64,\n",
    "                        label=\"Max Tokens\",\n",
    "                        info=\"Maximum length of generated response\"\n",
    "                    )\n",
    "                    \n",
    "                    top_p = gr.Slider(\n",
    "                        minimum=0.0, maximum=1.0, value=0.95, step=0.05,\n",
    "                        label=\"Top-p\",\n",
    "                        info=\"Nucleus sampling: consider tokens with top_p cumulative probability\"\n",
    "                    )\n",
    "                    \n",
    "                    top_k = gr.Slider(\n",
    "                        minimum=1, maximum=100, value=50, step=1,\n",
    "                        label=\"Top-k\",\n",
    "                        info=\"Consider top k most likely tokens at each step\"\n",
    "                    )\n",
    "                    \n",
    "                    repeat_penalty = gr.Slider(\n",
    "                        minimum=1.0, maximum=2.0, value=1.1, step=0.05,\n",
    "                        label=\"Repeat Penalty\",\n",
    "                        info=\"Higher values discourage repetition\"\n",
    "                    )\n",
    "                \n",
    "                # Threading\n",
    "                n_threads = gr.Slider(\n",
    "                    minimum=1, maximum=32, value=8, step=1,\n",
    "                    label=\"Number of Threads\",\n",
    "                    info=\"Number of CPU threads to use\"\n",
    "                )\n",
    "                \n",
    "                # Context size\n",
    "                n_ctx = gr.Slider(\n",
    "                    minimum=512, maximum=4096, value=2048, step=512,\n",
    "                    label=\"Context Size\",\n",
    "                    info=\"Maximum context window size\"\n",
    "                )\n",
    "                \n",
    "                # Model information\n",
    "                model_info = gr.Textbox(label=\"Model Status\", interactive=False)\n",
    "        \n",
    "        # Load the initial model\n",
    "        demo.load(lambda: f\"Enter model path and click 'Load Model'\", outputs=model_info)\n",
    "        \n",
    "        # Event handlers\n",
    "        load_btn = gr.Button(\"Load Model\")\n",
    "        load_btn.click(\n",
    "            lambda path, threads, ctx: load_model(path, threads, ctx)[1],\n",
    "            inputs=[model_path, n_threads, n_ctx],\n",
    "            outputs=model_info\n",
    "        )\n",
    "        \n",
    "        # Chat submission events\n",
    "        def respond(message, chat_history, system_prompt, temperature, max_tokens, top_p, top_k, repeat_penalty):\n",
    "            if not message.strip():\n",
    "                return chat_history\n",
    "            \n",
    "            # Add user message to history\n",
    "            chat_history.append((message, None))\n",
    "            yield chat_history\n",
    "            \n",
    "            # Get model response\n",
    "            response = chat_with_model(\n",
    "                message, chat_history[:-1], system_prompt,\n",
    "                temperature, max_tokens, top_p, top_k, repeat_penalty\n",
    "            )\n",
    "            \n",
    "            # Update history with bot response\n",
    "            chat_history[-1] = (message, response)\n",
    "            yield chat_history\n",
    "        \n",
    "        submit_btn.click(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, system_prompt, temperature, max_tokens, top_p, top_k, repeat_penalty],\n",
    "            outputs=chatbot,\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, system_prompt, temperature, max_tokens, top_p, top_k, repeat_penalty],\n",
    "            outputs=chatbot,\n",
    "            show_progress=True\n",
    "        ).then(\n",
    "            lambda: \"\",  # Clear the message box after sending\n",
    "            outputs=msg\n",
    "        )\n",
    "        \n",
    "        # Clear conversation button\n",
    "        clear_btn.click(\n",
    "            lambda: [],\n",
    "            outputs=chatbot\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"PLLuM-8x7B-chat Web UI\")\n",
    "    parser.add_argument(\"--model\", \"-m\", type=str, help=\"Path to the model file\")\n",
    "    parser.add_argument(\"--threads\", \"-t\", type=int, default=8, help=\"Number of threads to use\")\n",
    "    parser.add_argument(\"--ctx\", \"-c\", type=int, default=2048, help=\"Context size\")\n",
    "    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind to\")\n",
    "    parser.add_argument(\"--port\", \"-p\", type=int, default=7860, help=\"Port to bind to\")\n",
    "    parser.add_argument(\"--share\", \"-s\", action=\"store_true\", help=\"Create a public link\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Preload model if specified\n",
    "    if args.model:\n",
    "        print(f\"Loading model from {args.model}...\")\n",
    "        _, status = load_model(args.model, args.threads, args.ctx)\n",
    "        print(status)\n",
    "    \n",
    "    # Create and launch the web UI\n",
    "    demo = create_web_ui()\n",
    "    demo.launch(server_name=args.host, server_port=args.port, share=args.share)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Let's clean up any resources we may have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "if loaded_model is not None:\n",
    "    del loaded_model\n",
    "    loaded_model = None\n",
    "    loaded_model_name = None\n",
    "    \n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Resources released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've demonstrated how to create a web interface for the PLLuM-8x7B-chat model using Gradio. We've implemented:\n",
    "\n",
    "1. A chat interface with conversation history\n",
    "2. A model selection dropdown to switch between different quantization levels\n",
    "3. Adjustable generation parameters\n",
    "4. A translation-specific interface\n",
    "5. A standalone Python script for deployment\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "To run the standalone script from the command line:\n",
    "\n",
    "```bash\n",
    "python web_ui.py --model path/to/model.gguf --threads 8 --ctx 2048 --share\n",
    "```\n",
    "\n",
    "This will launch a web server that you can access locally, or publicly if you use the --share flag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
