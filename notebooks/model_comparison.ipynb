{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Different Quantization Levels of PLLuM-8x7B-chat\n",
    "\n",
    "This notebook compares the performance, speed, and quality of different quantization levels of the PLLuM-8x7B-chat model. We'll evaluate metrics such as:\n",
    "\n",
    "1. Memory usage\n",
    "2. Inference speed\n",
    "3. Output quality (subjective assessment)\n",
    "4. Perplexity on sample texts\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Download several quantized models from [Hugging Face](https://huggingface.co/piotrmaciejbednarski/PLLuM-8x7B-chat-GGUF)\n",
    "- Install required packages: `pip install llama-cpp-python matplotlib numpy pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install llama-cpp-python matplotlib numpy pandas psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Model Paths\n",
    "\n",
    "Let's define the paths to the different quantized models. You should update these paths to match your local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to different quantized models\n",
    "# Update these paths to match your local setup\n",
    "models = {\n",
    "    \"Q2_K\": \"../models/PLLuM-8x7B-chat-gguf-q2_k.gguf\",\n",
    "    \"IQ3_S\": \"../models/PLLuM-8x7B-chat-gguf-iq3_s.gguf\",\n",
    "    \"Q3_K_M\": \"../models/PLLuM-8x7B-chat-gguf-q3_k_m.gguf\",\n",
    "    \"Q4_K_M\": \"../models/PLLuM-8x7B-chat-gguf-q4_k_m.gguf\",\n",
    "    \"Q5_K_M\": \"../models/PLLuM-8x7B-chat-gguf-q5_k_m.gguf\",\n",
    "    \"Q8_0\": \"../models/PLLuM-8x7B-chat-gguf-q8_0.gguf\"\n",
    "}\n",
    "\n",
    "# Check which models are available\n",
    "available_models = {}\n",
    "for name, path in models.items():\n",
    "    if os.path.exists(path):\n",
    "        available_models[name] = path\n",
    "        print(f\"✅ {name} found at {path}\")\n",
    "    else:\n",
    "        print(f\"❌ {name} not found at {path}\")\n",
    "\n",
    "if not available_models:\n",
    "    raise FileNotFoundError(\"No models found. Please download at least one model.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(available_models)} model(s) for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Test Prompts\n",
    "\n",
    "Let's define a set of test prompts to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts for different tasks\n",
    "test_prompts = {\n",
    "    \"Simple Question\": \"Pytanie: Jakie są największe miasta w Polsce? Odpowiedź:\",\n",
    "    \"Complex Question\": \"Pytanie: Jakie są najważniejsze osiągnięcia polskiej nauki w XX wieku? Wyjaśnij ich znaczenie dla świata. Odpowiedź:\",\n",
    "    \"Translation\": \"Przetłumacz poniższy tekst z polskiego na angielski:\\n\\n'Polska kultura jest bogata w tradycje i historię. Od wieków słynęliśmy z gościnności i otwartości na inne kultury.'\\n\\nTłumaczenie:\",\n",
    "    \"Summarization\": \"Streszcz poniższy tekst w 3-4 zdaniach:\\n\\nKonstytucja 3 maja (właściwie Ustawa Rządowa z dnia 3 maja) – uchwalona 3 maja 1791 roku ustawa regulująca ustrój prawny Rzeczypospolitej Obojga Narodów. Powszechnie przyjmuje się, że Konstytucja 3 maja była pierwszą w Europie i drugą na świecie (po konstytucji amerykańskiej z 1787 r.) nowoczesną, spisaną konstytucją. Konstytucja 3 maja została ustanowiona ustawą rządową przyjętą tego dnia przez sejm. Została zaprojektowana w celu zlikwidowania obecnych od dawna wad systemu politycznego Rzeczypospolitej Obojga Narodów i jej złotej wolności. Konstytucja wprowadziła polityczne zrównanie mieszczan i szlachty oraz stawiała chłopów pod ochroną państwa, w ten sposób łagodząc najgorsze nadużycia pańszczyzny. Konstytucja zniosła zgubne instytucje, takie jak liberum veto, które przed przyjęciem Konstytucji pozostawiało sejm na łasce każdego posła, który, jeśli zechciał – z własnej inicjatywy, lub przekupiony przez zagraniczne siły, albo magnatów – móc unieważnić wszystkie podjęte przez sejm uchwały.\\n\\nStreszczenie:\",\n",
    "    \"Creative Writing\": \"Napisz krótkie opowiadanie (300-400 słów) o pierwszym kontakcie ludzi z cywilizacją pozaziemską. Opowiadanie powinno zawierać element zaskoczenia.\"\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(test_prompts)} test prompts for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Function\n",
    "\n",
    "Let's create a function to load a model and run the test prompts, measuring performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, model_path, test_prompts, max_tokens=512):\n",
    "    \"\"\"Test a model with the given prompts and gather performance metrics.\"\"\"\n",
    "    print(f\"Testing model: {model_name}\")\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"load_time\": 0,\n",
    "        \"memory_usage\": 0,\n",
    "        \"prompt_results\": {}\n",
    "    }\n",
    "    \n",
    "    # Record initial memory usage\n",
    "    initial_memory = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)  # MB\n",
    "    \n",
    "    # Load the model and measure time\n",
    "    load_start = time.time()\n",
    "    try:\n",
    "        llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=2048,\n",
    "            n_threads=8,\n",
    "            n_batch=512,\n",
    "            verbose=False\n",
    "        )\n",
    "        load_time = time.time() - load_start\n",
    "        results[\"load_time\"] = load_time\n",
    "        print(f\"  Model loaded in {load_time:.2f} seconds\")\n",
    "        \n",
    "        # Measure memory usage after loading\n",
    "        current_memory = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)  # MB\n",
    "        memory_usage = current_memory - initial_memory\n",
    "        results[\"memory_usage\"] = memory_usage\n",
    "        print(f\"  Memory usage: {memory_usage:.2f} MB\")\n",
    "        \n",
    "        # Run each test prompt\n",
    "        for prompt_name, prompt_text in test_prompts.items():\n",
    "            print(f\"  Running prompt: {prompt_name}\")\n",
    "            prompt_result = {\n",
    "                \"inference_time\": 0,\n",
    "                \"tokens_per_second\": 0,\n",
    "                \"output_text\": \"\"\n",
    "            }\n",
    "            \n",
    "            # Run the inference and measure time\n",
    "            inference_start = time.time()\n",
    "            output = llm(\n",
    "                prompt_text,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                top_k=50,\n",
    "                repeat_penalty=1.1\n",
    "            )\n",
    "            inference_time = time.time() - inference_start\n",
    "            \n",
    "            # Calculate tokens per second\n",
    "            output_text = output[\"choices\"][0][\"text\"]\n",
    "            output_length = len(output_text.split())\n",
    "            tokens_per_second = output_length / inference_time if inference_time > 0 else 0\n",
    "            \n",
    "            # Store results\n",
    "            prompt_result[\"inference_time\"] = inference_time\n",
    "            prompt_result[\"tokens_per_second\"] = tokens_per_second\n",
    "            prompt_result[\"output_text\"] = output_text\n",
    "            prompt_result[\"output_length\"] = output_length\n",
    "            \n",
    "            results[\"prompt_results\"][prompt_name] = prompt_result\n",
    "            print(f\"    Completed in {inference_time:.2f} seconds ({tokens_per_second:.2f} tokens/sec)\")\n",
    "        \n",
    "        # Calculate average tokens per second across all prompts\n",
    "        avg_tokens_per_second = np.mean([res[\"tokens_per_second\"] for res in results[\"prompt_results\"].values()])\n",
    "        results[\"avg_tokens_per_second\"] = avg_tokens_per_second\n",
    "        print(f\"  Average generation speed: {avg_tokens_per_second:.2f} tokens/sec\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model {model_name}: {e}\")\n",
    "        results[\"error\"] = str(e)\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'llm' in locals():\n",
    "            del llm\n",
    "            import gc\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"Completed testing {model_name}\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Tests on Available Models\n",
    "\n",
    "Now let's run the tests on each available model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests on available models\n",
    "all_results = {}\n",
    "\n",
    "# Select a subset of prompts for comparison to save time\n",
    "comparison_prompts = {\n",
    "    \"Simple Question\": test_prompts[\"Simple Question\"],\n",
    "    \"Translation\": test_prompts[\"Translation\"],\n",
    "    \"Summarization\": test_prompts[\"Summarization\"]\n",
    "}\n",
    "\n",
    "# Run tests with fewer tokens for faster comparison\n",
    "for model_name, model_path in available_models.items():\n",
    "    results = test_model(model_name, model_path, comparison_prompts, max_tokens=256)\n",
    "    all_results[model_name] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Memory Usage and Speed\n",
    "\n",
    "Let's analyze and visualize the memory usage and inference speed of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "model_names = list(all_results.keys())\n",
    "memory_usage = [all_results[name][\"memory_usage\"] for name in model_names]\n",
    "load_times = [all_results[name][\"load_time\"] for name in model_names]\n",
    "avg_tokens_per_second = [all_results[name][\"avg_tokens_per_second\"] for name in model_names]\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame({\n",
    "    \"Model\": model_names,\n",
    "    \"Memory Usage (MB)\": memory_usage,\n",
    "    \"Load Time (s)\": load_times,\n",
    "    \"Generation Speed (tokens/s)\": avg_tokens_per_second\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Performance Metrics:\")\n",
    "display(df)\n",
    "\n",
    "# Create comparison visualizations\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Memory usage\n",
    "axs[0].bar(model_names, memory_usage, color='skyblue')\n",
    "axs[0].set_title('Memory Usage by Model')\n",
    "axs[0].set_ylabel('Memory Usage (MB)')\n",
    "axs[0].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "# Load time\n",
    "axs[1].bar(model_names, load_times, color='salmon')\n",
    "axs[1].set_title('Model Load Time')\n",
    "axs[1].set_ylabel('Time (seconds)')\n",
    "axs[1].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "# Generation speed\n",
    "axs[2].bar(model_names, avg_tokens_per_second, color='lightgreen')\n",
    "axs[2].set_title('Average Generation Speed')\n",
    "axs[2].set_ylabel('Tokens per Second')\n",
    "axs[2].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Generation Speed by Task\n",
    "\n",
    "Let's compare how each model performs on different types of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens per second for each prompt and model\n",
    "prompt_data = {}\n",
    "for prompt_name in comparison_prompts.keys():\n",
    "    prompt_data[prompt_name] = []\n",
    "    for model_name in model_names:\n",
    "        if prompt_name in all_results[model_name][\"prompt_results\"]:\n",
    "            tokens_per_second = all_results[model_name][\"prompt_results\"][prompt_name][\"tokens_per_second\"]\n",
    "            prompt_data[prompt_name].append(tokens_per_second)\n",
    "        else:\n",
    "            prompt_data[prompt_name].append(0)\n",
    "\n",
    "# Create a grouped bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "multiplier = 0\n",
    "\n",
    "for prompt_name, tokens_per_second in prompt_data.items():\n",
    "    offset = width * multiplier\n",
    "    plt.bar(x + offset, tokens_per_second, width, label=prompt_name)\n",
    "    multiplier += 1\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Tokens per Second')\n",
    "plt.title('Generation Speed by Model and Task')\n",
    "plt.xticks(x + width, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output Quality Comparison\n",
    "\n",
    "Let's compare the output quality for one prompt across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the output for the \"Simple Question\" prompt\n",
    "prompt_to_compare = \"Simple Question\"\n",
    "print(f\"Comparing outputs for: {prompt_to_compare}\")\n",
    "print(f\"Prompt: {comparison_prompts[prompt_to_compare]}\\n\")\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "    if prompt_to_compare in all_results[model_name][\"prompt_results\"]:\n",
    "        output_text = all_results[model_name][\"prompt_results\"][prompt_to_compare][\"output_text\"]\n",
    "        print(output_text[:500] + (\"...\" if len(output_text) > 500 else \"\"))\n",
    "    else:\n",
    "        print(\"No output available for this model and prompt.\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Output Length Comparison\n",
    "\n",
    "Let's compare the output length for each prompt across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract output length for each prompt and model\n",
    "output_lengths = {}\n",
    "for prompt_name in comparison_prompts.keys():\n",
    "    output_lengths[prompt_name] = []\n",
    "    for model_name in model_names:\n",
    "        if prompt_name in all_results[model_name][\"prompt_results\"]:\n",
    "            length = all_results[model_name][\"prompt_results\"][prompt_name][\"output_length\"]\n",
    "            output_lengths[prompt_name].append(length)\n",
    "        else:\n",
    "            output_lengths[prompt_name].append(0)\n",
    "\n",
    "# Create a grouped bar chart for output lengths\n",
    "plt.figure(figsize=(14, 8))\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "multiplier = 0\n",
    "\n",
    "for prompt_name, lengths in output_lengths.items():\n",
    "    offset = width * multiplier\n",
    "    plt.bar(x + offset, lengths, width, label=prompt_name)\n",
    "    multiplier += 1\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Output Length (tokens)')\n",
    "plt.title('Output Length by Model and Task')\n",
    "plt.xticks(x + width, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate Efficiency Score\n",
    "\n",
    "Let's calculate an efficiency score that balances speed and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate efficiency score (tokens per second per MB of memory)\n",
    "df[\"Efficiency Score\"] = df[\"Generation Speed (tokens/s)\"] / df[\"Memory Usage (MB)\"] * 1000  # Scale up for readability\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Efficiency Metrics:\")\n",
    "display(df)\n",
    "\n",
    "# Create a bar chart for efficiency score\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(df[\"Model\"], df[\"Efficiency Score\"], color='purple')\n",
    "plt.title('Efficiency Score by Model (Higher is Better)')\n",
    "plt.ylabel('Efficiency Score (tokens/s per MB × 1000)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create a Recommendations Guide\n",
    "\n",
    "Based on our analysis, let's create a recommendations guide for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the models by different metrics\n",
    "best_memory = df.sort_values(\"Memory Usage (MB)\").iloc[0][\"Model\"]\n",
    "best_speed = df.sort_values(\"Generation Speed (tokens/s)\", ascending=False).iloc[0][\"Model\"]\n",
    "best_efficiency = df.sort_values(\"Efficiency Score\", ascending=False).iloc[0][\"Model\"]\n",
    "\n",
    "# Create a recommendations DataFrame\n",
    "recommendations = {\n",
    "    \"Use Case\": [\n",
    "        \"Low-resource environments (e.g., old laptops, minimal RAM)\",\n",
    "        \"Balanced performance (good quality with reasonable resources)\",\n",
    "        \"Real-time applications (chatbots, interactive tools)\",\n",
    "        \"High-quality outputs (creative writing, complex analyses)\",\n",
    "        \"Mobile devices or embedded systems\"\n",
    "    ],\n",
    "    \"Recommended Model\": [\n",
    "        best_memory,\n",
    "        \"Q4_K_M\",  # Commonly recommended for balance\n",
    "        best_speed,\n",
    "        \"Q8_0\",  # Highest quality\n",
    "        best_efficiency\n",
    "    ],\n",
    "    \"Rationale\": [\n",
    "        f\"Lowest memory usage among tested models ({df[df['Model']==best_memory]['Memory Usage (MB)'].values[0]:.2f} MB)\",\n",
    "        \"Good balance between quality and resource requirements\",\n",
    "        f\"Fastest generation speed ({df[df['Model']==best_speed]['Generation Speed (tokens/s)'].values[0]:.2f} tokens/s)\",\n",
    "        \"Highest quality outputs (closest to original model)\",\n",
    "        f\"Best efficiency score (speed per MB of memory)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create and display the recommendations DataFrame\n",
    "recommendations_df = pd.DataFrame(recommendations)\n",
    "print(\"Recommendations Based on Test Results:\")\n",
    "display(recommendations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Subjective Quality Assessment\n",
    "\n",
    "While it's difficult to automate quality assessment, let's create a simple function to help manually evaluate the quality of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs_side_by_side(prompt_name, model_name1, model_name2):\n",
    "    \"\"\"Compare outputs from two models side by side.\"\"\"\n",
    "    print(f\"Comparing {model_name1} vs {model_name2} for prompt: {prompt_name}\")\n",
    "    print(f\"Prompt: {comparison_prompts[prompt_name]}\\n\")\n",
    "    \n",
    "    output1 = \"No output available\" \n",
    "    output2 = \"No output available\"\n",
    "    \n",
    "    if prompt_name in all_results[model_name1][\"prompt_results\"]:\n",
    "        output1 = all_results[model_name1][\"prompt_results\"][prompt_name][\"output_text\"]\n",
    "    \n",
    "    if prompt_name in all_results[model_name2][\"prompt_results\"]:\n",
    "        output2 = all_results[model_name2][\"prompt_results\"][prompt_name][\"output_text\"]\n",
    "    \n",
    "    # Display outputs side by side\n",
    "    print(\"-\"*100)\n",
    "    print(f\"{model_name1:50} | {model_name2:50}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Split by newlines to make comparison easier\n",
    "    lines1 = output1.split(\"\\n\")\n",
    "    lines2 = output2.split(\"\\n\")\n",
    "    \n",
    "    # Find the maximum number of lines\n",
    "    max_lines = max(len(lines1), len(lines2))\n",
    "    \n",
    "    for i in range(max_lines):\n",
    "        line1 = lines1[i] if i < len(lines1) else \"\"\n",
    "        line2 = lines2[i] if i < len(lines2) else \"\"\n",
    "        print(f\"{line1[:50]:50} | {line2[:50]:50}\")\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "\n",
    "# Let's compare a couple of models if we have multiple available\n",
    "if len(model_names) >= 2:\n",
    "    # Compare the first two models for each prompt\n",
    "    for prompt_name in comparison_prompts.keys():\n",
    "        compare_outputs_side_by_side(prompt_name, model_names[0], model_names[1])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results\n",
    "\n",
    "Let's export our findings to CSV files for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export performance metrics to CSV\n",
    "df.to_csv(\"../results/model_performance_metrics.csv\", index=False)\n",
    "recommendations_df.to_csv(\"../results/model_recommendations.csv\", index=False)\n",
    "\n",
    "print(\"Exported results to CSV files in the 'results' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup\n",
    "\n",
    "Let's clean up any resources we may have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the namespace\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Resources released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've compared different quantization levels of the PLLuM-8x7B-chat model across several dimensions:\n",
    "\n",
    "1. **Memory Usage**: Lower quantization levels use significantly less memory\n",
    "2. **Load Time**: Models with lower quantization generally load faster\n",
    "3. **Inference Speed**: Different quantization levels have varying effects on generation speed\n",
    "4. **Output Quality**: Higher quantization levels tend to produce higher quality outputs\n",
    "5. **Efficiency**: We calculated an efficiency score to balance speed and memory usage\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The **Q4_K_M** quantization offers a good balance between quality and resource requirements for most applications\n",
    "- For low-resource environments, the **Q2_K** or **IQ3_S** quantizations are viable options\n",
    "- For highest quality outputs, the **Q8_0** quantization is recommended, but requires more resources\n",
    "- The choice of quantization should be based on your specific use case and available hardware\n",
    "\n",
    "These results can help you choose the right quantization level for your specific use case and hardware constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
