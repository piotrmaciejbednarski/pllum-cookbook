{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Completion with PLLuM-8x7B-chat GGUF\n",
    "\n",
    "This notebook demonstrates how to implement a chat completion interface with the PLLuM-8x7B-chat model, including handling conversation history.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Download a quantized model from [Hugging Face](https://huggingface.co/piotrmaciejbednarski/PLLuM-8x7B-chat-GGUF)\n",
    "- Install required packages: `pip install llama-cpp-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "First, we'll load the model using the `Llama` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "# Set path to the model file - update this to your model's location\n",
    "model_path = \"../models/PLLuM-8x7B-chat-gguf-q4_k_m.gguf\"\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}. Please download it first.\")\n",
    "\n",
    "# Load the model\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,      # Context window size\n",
    "    n_threads=8,     # Number of CPU threads to use\n",
    "    n_batch=512,     # Batch size for prompt processing\n",
    "    verbose=False    # Set to True for more detailed logs\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a Chat Interface\n",
    "\n",
    "Let's implement a simple chat interface that keeps track of conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self, llm, system_prompt=None):\n",
    "        self.llm = llm\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Set default system prompt if none provided\n",
    "        if system_prompt is None:\n",
    "            self.system_prompt = \"Jesteś pomocnym, uprzejmym i dokładnym asystentem AI o nazwie PLLuM. \"\\\n",
    "                                \"Odpowiadasz na pytania użytkownika w języku polskim w sposób przyjazny i profesjonalny.\"\n",
    "        else:\n",
    "            self.system_prompt = system_prompt\n",
    "            \n",
    "        # Add system prompt to history\n",
    "        self.conversation_history.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
    "    \n",
    "    def get_prompt_template(self):\n",
    "        \"\"\"Create a formatted prompt from the conversation history.\"\"\"\n",
    "        prompt = \"\"\n",
    "        \n",
    "        for message in self.conversation_history:\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"]\n",
    "            \n",
    "            if role == \"system\":\n",
    "                # System messages are typically not shown in the prompt template\n",
    "                # but can be included at the beginning\n",
    "                if prompt == \"\":\n",
    "                    prompt += f\"Instrukcja: {content}\\n\\n\"\n",
    "            elif role == \"user\":\n",
    "                prompt += f\"Użytkownik: {content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                prompt += f\"Asystent: {content}\\n\"\n",
    "        \n",
    "        # Add the assistant prefix for the response\n",
    "        prompt += \"Asystent: \"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def chat(self, user_message, max_tokens=512, temperature=0.7):\n",
    "        \"\"\"Process a user message and generate a response.\"\"\"\n",
    "        # Add the user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Create the prompt from conversation history\n",
    "        prompt = self.get_prompt_template()\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.llm(\n",
    "            prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repeat_penalty=1.1,\n",
    "            stop=[\"\\nUżytkownik:\", \"\\nUser:\"]  # Stop generating at new user message\n",
    "        )\n",
    "        \n",
    "        assistant_response = response[\"choices\"][0][\"text\"]\n",
    "        \n",
    "        # Add the assistant response to history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "        \n",
    "        return assistant_response\n",
    "    \n",
    "    def reset_conversation(self, system_prompt=None):\n",
    "        \"\"\"Reset the conversation history.\"\"\"\n",
    "        if system_prompt is not None:\n",
    "            self.system_prompt = system_prompt\n",
    "            \n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "        \n",
    "    def view_conversation_history(self):\n",
    "        \"\"\"Display the conversation history.\"\"\"\n",
    "        for message in self.conversation_history:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                print(f\"[System] {message['content']}\")\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                print(f\"[User] {message['content']}\")\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                print(f\"[Assistant] {message['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Test the Chat Bot\n",
    "\n",
    "Let's create a chatbot instance and test it with a few messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot with a custom system prompt\n",
    "system_prompt = (\n",
    "    \"Jesteś pomocnym, uprzejmym i dokładnym asystentem AI o imieniu PLLuM, \"\n",
    "    \"stworzonym przez polskie Ministerstwo Cyfryzacji. \"\n",
    "    \"Odpowiadasz na pytania użytkownika w języku polskim, chyba że zostaniesz poproszony o inny język. \"\n",
    "    \"Twoje odpowiedzi są zwięzłe, merytoryczne i pomocne. \"\n",
    "    \"Jeśli nie znasz odpowiedzi na pytanie, przyznaj to otwarcie.\"\n",
    ")\n",
    "\n",
    "chatbot = ChatBot(llm, system_prompt=\"\".join(system_prompt))\n",
    "\n",
    "# Test the first message\n",
    "user_message = \"Cześć! Jak się masz?\"\n",
    "print(f\"[User] {user_message}\")\n",
    "\n",
    "response = chatbot.chat(user_message)\n",
    "print(f\"[Assistant] {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Continue the Conversation\n",
    "\n",
    "Let's continue the conversation with a few more messages to test context retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow-up question\n",
    "user_message = \"Powiedz mi coś o sobie. Kim jesteś?\"\n",
    "print(f\"[User] {user_message}\")\n",
    "\n",
    "response = chatbot.chat(user_message)\n",
    "print(f\"[Assistant] {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for some information\n",
    "user_message = \"Możesz mi wyjaśnić, co to jest sztuczna inteligencja?\"\n",
    "print(f\"[User] {user_message}\")\n",
    "\n",
    "response = chatbot.chat(user_message)\n",
    "print(f\"[Assistant] {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the previous conversation\n",
    "user_message = \"Czy możesz podać mi kilka przykładów zastosowań sztucznej inteligencji w codziennym życiu?\"\n",
    "print(f\"[User] {user_message}\")\n",
    "\n",
    "response = chatbot.chat(user_message)\n",
    "print(f\"[Assistant] {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View the Complete Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the complete conversation\n",
    "print(\"Complete Conversation History:\")\n",
    "print(\"-\" * 50)\n",
    "chatbot.view_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reset and Start a New Conversation\n",
    "\n",
    "Let's reset the conversation and start a new one with a different system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the conversation with a new system prompt\n",
    "new_system_prompt = (\n",
    "    \"Jesteś ekspertem w dziedzinie historii Polski. \"\n",
    "    \"Odpowiadasz na pytania w sposób rzeczowy, podając istotne fakty, daty i kontekst historyczny. \"\n",
    "    \"Twoje odpowiedzi są zwięzłe, ale bogate w informacje.\"\n",
    ")\n",
    "\n",
    "chatbot.reset_conversation(system_prompt=new_system_prompt)\n",
    "print(\"Conversation has been reset with a new system prompt.\")\n",
    "\n",
    "# Test the new conversation\n",
    "user_message = \"Opowiedz mi o najważniejszych wydarzeniach w historii Polski XX wieku.\"\n",
    "print(f\"[User] {user_message}\")\n",
    "\n",
    "response = chatbot.chat(user_message, max_tokens=1024)\n",
    "print(f\"[Assistant] {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementing a Simple Interactive Chat Loop\n",
    "\n",
    "Let's create a simple interactive chat interface using IPython widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create an interactive chat interface\n",
    "def interactive_chat():\n",
    "    # Reset chatbot with general purpose prompt\n",
    "    general_system_prompt = \"Jesteś pomocnym, uprzejmym i dokładnym asystentem AI o nazwie PLLuM. Odpowiadasz na pytania w sposób przyjazny i profesjonalny.\"\n",
    "    chatbot.reset_conversation(system_prompt=general_system_prompt)\n",
    "    \n",
    "    # Chat history display\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Text input for user messages\n",
    "    text_input = widgets.Text(\n",
    "        placeholder='Wpisz wiadomość...',\n",
    "        description='',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    \n",
    "    # Send button\n",
    "    send_button = widgets.Button(\n",
    "        description='Wyślij',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='18%')\n",
    "    )\n",
    "    \n",
    "    # Reset button\n",
    "    reset_button = widgets.Button(\n",
    "        description='Reset',\n",
    "        button_style='danger',\n",
    "        layout=widgets.Layout(width='18%')\n",
    "    )\n",
    "    \n",
    "    # Layout for input area\n",
    "    input_area = widgets.HBox([text_input, send_button, reset_button])\n",
    "    \n",
    "    # Display welcome message\n",
    "    with output:\n",
    "        print(\"Welcome to PLLuM Chat! Type a message and press 'Send' or hit Enter.\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Message handler\n",
    "    def on_send(b):\n",
    "        user_message = text_input.value\n",
    "        if not user_message.strip():\n",
    "            return\n",
    "            \n",
    "        # Clear the input field\n",
    "        text_input.value = ''\n",
    "        \n",
    "        with output:\n",
    "            print(f\"\\n[User] {user_message}\")\n",
    "            print(\"[Assistant] Thinking...\")\n",
    "            \n",
    "            # Get response from chatbot\n",
    "            response = chatbot.chat(user_message)\n",
    "            \n",
    "            # Clear the \"thinking\" message and display the response\n",
    "            clear_output(wait=True)\n",
    "            chatbot.view_conversation_history()\n",
    "    \n",
    "    # Reset handler\n",
    "    def on_reset(b):\n",
    "        chatbot.reset_conversation()\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(\"Conversation has been reset.\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Handle Enter key\n",
    "    def on_enter(sender):\n",
    "        on_send(None)\n",
    "    \n",
    "    # Attach event handlers\n",
    "    send_button.on_click(on_send)\n",
    "    reset_button.on_click(on_reset)\n",
    "    text_input.on_submit(on_enter)\n",
    "    \n",
    "    # Display the widget\n",
    "    display(widgets.VBox([output, input_area]))\n",
    "\n",
    "# Launch the interactive chat\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling Special Instructions\n",
    "\n",
    "Let's create a version of the chat interface that can handle special instructions for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedChatBot(ChatBot):\n",
    "    def __init__(self, llm, system_prompt=None):\n",
    "        super().__init__(llm, system_prompt)\n",
    "        self.special_instructions = {}\n",
    "    \n",
    "    def add_instruction(self, key, instruction):\n",
    "        \"\"\"Add a special instruction to the chatbot.\"\"\"\n",
    "        self.special_instructions[key] = instruction\n",
    "        print(f\"Instruction '{key}' added.\")\n",
    "    \n",
    "    def remove_instruction(self, key):\n",
    "        \"\"\"Remove a special instruction from the chatbot.\"\"\"\n",
    "        if key in self.special_instructions:\n",
    "            del self.special_instructions[key]\n",
    "            print(f\"Instruction '{key}' removed.\")\n",
    "        else:\n",
    "            print(f\"Instruction '{key}' not found.\")\n",
    "    \n",
    "    def chat(self, user_message, max_tokens=512, temperature=0.7, instruction_key=None):\n",
    "        \"\"\"Process a user message with optional special instruction.\"\"\"\n",
    "        # Use specific instruction if provided\n",
    "        temp_system_prompt = None\n",
    "        if instruction_key and instruction_key in self.special_instructions:\n",
    "            # Save the original system prompt\n",
    "            original_system_prompt = self.conversation_history[0][\"content\"]\n",
    "            \n",
    "            # Update with the special instruction\n",
    "            temp_system_prompt = self.special_instructions[instruction_key]\n",
    "            self.conversation_history[0][\"content\"] = temp_system_prompt\n",
    "        \n",
    "        # Call the parent chat method\n",
    "        response = super().chat(user_message, max_tokens, temperature)\n",
    "        \n",
    "        # Restore the original system prompt if it was changed\n",
    "        if temp_system_prompt is not None:\n",
    "            self.conversation_history[0][\"content\"] = original_system_prompt\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create an advanced chatbot\n",
    "advanced_chatbot = AdvancedChatBot(llm)\n",
    "\n",
    "# Add some special instructions\n",
    "advanced_chatbot.add_instruction(\"poet\", \"Jesteś poetą. Odpowiadasz w formie wiersza, używając metafor i barwnego języka.\")\n",
    "advanced_chatbot.add_instruction(\"historian\", \"Jesteś historykiem. Odpowiadasz w szczegółowy sposób, podając daty, fakty i kontekst historyczny.\")\n",
    "advanced_chatbot.add_instruction(\"concise\", \"Odpowiadasz bardzo zwięźle, używając maksymalnie 3 zdań.\")\n",
    "\n",
    "# Test with different instructions\n",
    "questions = [\n",
    "    \"Opowiedz mi o Warszawie.\",\n",
    "    \"Czym jest szczęście?\",\n",
    "    \"Jakie są najpopularniejsze sporty w Polsce?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\n{'=' * 50}\\nQuestion: {question}\\n{'=' * 50}\\n\")\n",
    "    \n",
    "    print(\"[Standard response]\")\n",
    "    response = advanced_chatbot.chat(question)\n",
    "    print(response)\n",
    "    \n",
    "    print(\"\\n[Poet response]\")\n",
    "    response = advanced_chatbot.chat(question, instruction_key=\"poet\")\n",
    "    print(response)\n",
    "    \n",
    "    print(\"\\n[Concise response]\")\n",
    "    response = advanced_chatbot.chat(question, instruction_key=\"concise\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "When you're done with the model, it's a good practice to clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the model from memory\n",
    "del llm\n",
    "del chatbot\n",
    "del advanced_chatbot\n",
    "\n",
    "# Force Python's garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Model resources released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've demonstrated how to implement a chat completion interface with the PLLuM-8x7B-chat model in GGUF format. We've shown:\n",
    "\n",
    "1. How to create a basic chatbot with conversation history\n",
    "2. How to format prompts for chat completion\n",
    "3. How to use system prompts to guide the model's behavior\n",
    "4. How to implement special instructions for different response styles\n",
    "5. How to create an interactive chat interface\n",
    "\n",
    "These techniques can be extended and customized for various applications, such as customer service chatbots, language learning assistants, or specialized knowledge agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
