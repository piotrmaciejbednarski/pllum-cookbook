{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Inference with PLLuM-8x7B-chat GGUF\n",
    "\n",
    "This notebook demonstrates how to use the quantized PLLuM model for basic text generation using the `llama-cpp-python` library.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Download a quantized model from [Hugging Face](https://huggingface.co/piotrmaciejbednarski/PLLuM-8x7B-chat-GGUF)\n",
    "- Install required packages: `pip install llama-cpp-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model\n",
    "\n",
    "First, we'll load the model using the `Llama` class from `llama_cpp`. You can adjust the parameters based on your hardware capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "# Set path to the model file - update this to your model's location\n",
    "model_path = \"../models/PLLuM-8x7B-chat-gguf-q4_k_m.gguf\"\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}. Please download it first.\")\n",
    "\n",
    "# Load the model\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,      # Context window size\n",
    "    n_threads=8,     # Number of CPU threads to use\n",
    "    n_batch=512,     # Batch size for prompt processing\n",
    "    verbose=False    # Set to True for more detailed logs\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Text Generation\n",
    "\n",
    "Now, let's generate some text with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple prompt in Polish\n",
    "prompt = \"Pytanie: Jakie są największe miasta w Polsce? Odpowiedź:\"\n",
    "\n",
    "# Generate text\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=512,      # Maximum number of tokens to generate\n",
    "    temperature=0.7,     # Controls randomness (lower = more deterministic)\n",
    "    top_p=0.95,          # Top-p sampling\n",
    "    top_k=50,            # Top-k sampling\n",
    "    repeat_penalty=1.1   # Penalty for repeating tokens\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(prompt + output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Different Prompts\n",
    "\n",
    "Let's test the model with a few different types of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"Generate text using the loaded model.\"\"\"\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        repeat_penalty=1.1\n",
    "    )\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Example 1: Factual question\n",
    "prompt1 = \"Pytanie: Kto był pierwszym prezydentem Polski po 1989 roku? Odpowiedź:\"\n",
    "result1 = generate_text(prompt1)\n",
    "print(f\"Prompt: {prompt1}\\n\\nGenerated: {result1}\\n\\n{'-'*80}\\n\")\n",
    "\n",
    "# Example 2: Summary request\n",
    "prompt2 = \"Napisz krótkie streszczenie bitwy pod Grunwaldem.\"\n",
    "result2 = generate_text(prompt2)\n",
    "print(f\"Prompt: {prompt2}\\n\\nGenerated: {result2}\\n\\n{'-'*80}\\n\")\n",
    "\n",
    "# Example 3: Creative writing\n",
    "prompt3 = \"Napisz krótkie opowiadanie o pierwszym kontakcie ludzi z obcą cywilizacją.\"\n",
    "result3 = generate_text(prompt3, max_tokens=1024, temperature=0.8)\n",
    "print(f\"Prompt: {prompt3}\\n\\nGenerated: {result3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring Temperature Parameter\n",
    "\n",
    "Let's explore how the temperature parameter affects text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Dokończ zdanie: W przyszłości sztuczna inteligencja będzie\"\n",
    "\n",
    "# Generate with different temperatures\n",
    "temperatures = [0.3, 0.7, 1.2]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n### Temperature = {temp} ###\\n\")\n",
    "    for _ in range(3):  # Generate 3 samples at each temperature\n",
    "        result = generate_text(prompt, max_tokens=100, temperature=temp)\n",
    "        print(f\"Sample: {prompt}{result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Polish-English Translation Example\n",
    "\n",
    "Let's test the model's ability to translate between Polish and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polish to English\n",
    "pl_to_en_prompt = \"Przetłumacz poniższy tekst z języka polskiego na angielski:\\n\\n'Warszawa jest stolicą Polski i największym miastem w kraju. Położona jest nad rzeką Wisłą i ma bogatą historię.'\\n\\nTłumaczenie:\"\n",
    "pl_to_en_result = generate_text(pl_to_en_prompt)\n",
    "print(f\"Polish to English:\\n{pl_to_en_prompt}\\n{pl_to_en_result}\\n\\n{'-'*80}\\n\")\n",
    "\n",
    "# English to Polish\n",
    "en_to_pl_prompt = \"Translate the following text from English to Polish:\\n\\n'Poland is a country with a rich cultural heritage, beautiful landscapes, and delicious cuisine. It is located in Central Europe.'\\n\\nTłumaczenie:\"\n",
    "en_to_pl_result = generate_text(en_to_pl_prompt)\n",
    "print(f\"English to Polish:\\n{en_to_pl_prompt}\\n{en_to_pl_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Usage and Performance\n",
    "\n",
    "Let's check the memory usage and performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get the current memory usage of the process in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# Measure generation time and memory usage\n",
    "prompt = \"Napisz pięć ciekawostek o kosmosie.\"\n",
    "\n",
    "# Record starting memory\n",
    "start_memory = get_memory_usage()\n",
    "print(f\"Starting memory usage: {start_memory:.2f} MB\")\n",
    "\n",
    "# Measure generation time\n",
    "start_time = time.time()\n",
    "result = generate_text(prompt, max_tokens=256)\n",
    "end_time = time.time()\n",
    "\n",
    "# Record ending memory\n",
    "end_memory = get_memory_usage()\n",
    "\n",
    "# Calculate statistics\n",
    "generation_time = end_time - start_time\n",
    "memory_increase = end_memory - start_memory\n",
    "\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Current memory usage: {end_memory:.2f} MB\")\n",
    "print(f\"Memory increase during generation: {memory_increase:.2f} MB\")\n",
    "print(f\"\\nGenerated text:\\n{prompt}{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "When you're done with the model, it's a good practice to clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the model from memory\n",
    "del llm\n",
    "\n",
    "# Force Python's garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"Model resources released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows good performance across various tasks including:\n",
    "\n",
    "- Answering factual questions\n",
    "- Summarizing historical events\n",
    "- Creative writing\n",
    "- Translation between Polish and English\n",
    "\n",
    "Experiment with different quantization levels and parameters to find the best balance between performance and quality for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
